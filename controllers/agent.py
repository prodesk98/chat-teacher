from asyncio import to_thread

from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from sqlalchemy.orm import Session

from config import env
from databases.sessions import Chat
from .chat import ChatController
from prompts import TEACHER_AGENT_TEMPLATE


class AgentController:
    def __init__(self, db: Session):
        self._db = db
        self._llm = ChatOpenAI(
            base_url=env.OPENAI_BASE_URL,
            api_key=env.OPENAI_API_KEY,
            model=env.OPENAI_MODEL,
            temperature=.6,
        )

    def _llm_generate(self, messages: list[BaseMessage]) -> str:
        template = ChatPromptTemplate.from_messages(
            [
                SystemMessage(content=TEACHER_AGENT_TEMPLATE),
                *messages,
            ]
        )
        chain = template | self._llm
        return chain.invoke({}).content

    def generate(self, chat_id: int) -> str:
        """
        Generate a response for the given chat ID and message.
        This method should interact with the OpenAI API to generate a response.
        """
        try:
            chat = self._db.query(Chat).filter(Chat.id == chat_id).first()
            if not chat:
                raise ValueError(f"Chat with ID {chat_id} not found.")
            content = self._llm_generate([
                HumanMessage(m.content) if m.role == 'user' else AIMessage(m.content)
                for m in chat.messages
            ])
            if not content:
                raise ValueError("No content generated by the model.")
            ChatController(self._db).create_message(chat_id, role='assistant', content=content)
            return content
        except Exception as e:
            raise RuntimeError(f"Database error: {str(e)}")
        finally:
            self._db.close()

    async def agenerate(self, chat_id: int) -> str:
        return await to_thread(self.generate, chat_id)
